{
 "metadata": {
  "name": "",
  "signature": "sha256:e71c6f879ac982fb520e1e720b7f5a06a3ee0b6de847386af383c31e6cfa3765"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!pip install nltk"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\u001b[33mYou are using pip version 6.0.7, however version 6.0.8 is available.\r\n",
        "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\r\n",
        "Requirement already satisfied (use --upgrade to upgrade): nltk in /usr/local/lib/python2.7/site-packages\r\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.tokenize import sent_tokenize"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.tokenize import TreebankWordTokenizer\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# import nltk\n",
      "# nltk.download()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 85
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text = \"Hello. How are you, dear sir? Are you well? Here: drink this! It will make you feel better.\"\n",
      "\n",
      "sentences = sent_tokenize(text)\n",
      "sentences"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "['Hello.',\n",
        " 'How are you, dear sir?',\n",
        " 'Are you well?',\n",
        " 'Here: drink this!',\n",
        " 'It will make you feel better.']"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tokenizer = TreebankWordTokenizer()\n",
      "tokenizer.tokenize(sentences[4])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "['It', 'will', 'make', 'you', 'feel', 'better', '.']"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.tokenize import word_tokenize\n",
      "\n",
      "words = word_tokenize(sentences[4])\n",
      "words"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "['It', 'will', 'make', 'you', 'feel', 'better', '.']"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "word_tokenize(sentences[3])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "['Here', ':', 'drink', 'this', '!']"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "word_tokenize(\"Who's going to that thing today?\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "['Who', \"'s\", 'going', 'to', 'that', 'thing', 'today', '?']"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.tokenize import wordpunct_tokenize\n",
      "wordpunct_tokenize(\"Who's going to that thing today?\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "['Who', \"'\", 's', 'going', 'to', 'that', 'thing', 'today', '?']"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.tag import pos_tag\n",
      "words = word_tokenize(\"Who's going to that thing today?\")\n",
      "pos_tag(words)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "[('Who', 'WP'),\n",
        " (\"'s\", 'VBZ'),\n",
        " ('going', 'VBG'),\n",
        " ('to', 'TO'),\n",
        " ('that', 'DT'),\n",
        " ('thing', 'NN'),\n",
        " ('today', 'NN'),\n",
        " ('?', '.')]"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# import nltk\n",
      "# nltk.download()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 86
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# from nltk.corpus import treebank_chunk\n",
      "# treebank_chunk.tagged_sents()[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 87
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "treebank_chunk.chunked_sents()[0].draw"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "<bound method Tree.draw of Tree('S', [Tree('NP', [(u'Pierre', u'NNP'), (u'Vinken', u'NNP')]), (u',', u','), Tree('NP', [(u'61', u'CD'), (u'years', u'NNS')]), (u'old', u'JJ'), (u',', u','), (u'will', u'MD'), (u'join', u'VB'), Tree('NP', [(u'the', u'DT'), (u'board', u'NN')]), (u'as', u'IN'), Tree('NP', [(u'a', u'DT'), (u'nonexecutive', u'JJ'), (u'director', u'NN'), (u'Nov.', u'NNP'), (u'29', u'CD')]), (u'.', u'.')])>"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!pip install textblob"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 88
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from textblob import TextBlob"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "GATSBY_TEXT = \"\"\"In my younger and more vulnerable years my father gave me some advice that I've been turning over in my mind ever since. \"Whenever you feel like criticizing any one\", he told me, \"blah blah, blah blah blah.\" \"\"\"\n",
      "\n",
      "gatsby = TextBlob(GATSBY_TEXT)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "TextBlob(\"oh my god, I love this bootcamp, it is so awesome.\").sentiment"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 37,
       "text": [
        "Sentiment(polarity=0.75, subjectivity=0.8)"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gatsby.noun_phrases"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 43,
       "text": [
        "WordList([u'vulnerable years', 'whenever', u'blah blah', u'blah blah blah'])"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gatsby.sentiment"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 44,
       "text": [
        "Sentiment(polarity=0.0, subjectivity=0.3333333333333333)"
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# import nltk\n",
      "# stemmer = nltk.stem.PorterStemmer()\n",
      "\n",
      "# for word in TextBlob(\"oh my god, I was going to go to this bootcamp, it is so awesome.\").words:\n",
      "#     print stemmer.stem(word)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 84
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for word,count in sorted(gatsby.word_counts.items(), key=lambda item: item[1], reverse=True):\n",
      "    print \"%15s\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# from collections import Counter\n",
      "\n",
      "# stemmer = nltk.stem.porter.PorterStemmer()\n",
      "# stem_counts = Counter()\n",
      "\n",
      "# for word in gatsby.words:\n",
      "#     stem = stemmer.stem(word)\n",
      "#     stem_counts[stem] +=1\n",
      "    \n",
      "# for stem in stem_counts.most_common(100):\n",
      "#     print stem"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 82
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "consumer_key = \"Xj4Xbph5rstwLgEuUXY8tugpz\"\n",
      "consumer_secret = \"jfjAKLx2vOwKR4a7fOPeWIEzwgH2JbRLDq0G5duJdVnWcNAXLR\"\n",
      "access_token = \"249061702-eJnbhA4qDZFWWRq517NBvGVzCconkiRfQfwVlGT4\"\n",
      "access_secret = \"ryI0rltBukG9moFuJLPnJQxhJYXXkzt2vHRebo9qTaUoE\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pymongo"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "c = pymongo.MongoClient()\n",
      "db = c['bitcoindb']\n",
      "tweets = db['tweets']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 89
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import tweepy\n",
      "\n",
      "search_results "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import requests\n",
      "from requests_oauthlib import OAuth1\n",
      "\n",
      "\n",
      "oauth = OAuth1(consumer_key, \n",
      "                client_secret=consumer_secret,\n",
      "                resource_owner_key=access_token, \n",
      "                resource_owner_secret = access_secret)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "response = requests.get(\"https://api.twitter.com/1.1/statuses/user_timeline.json\", auth=oauth)\n",
      "#print response.text\n",
      "tweets =response.json()\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# search_url = \"https://api.twitter.com/1.1/search/tweets.json\"\n",
      "\n",
      "# parameters = {\"q\": \"bitcoin\", \"count\":100}\n",
      "# response = requests.get(search_url, params=parameters, auth=oauth)\n",
      "# tweets = response.json()['statuses']\n",
      "\n",
      "# count =0 \n",
      "# print 'Page 1'\n",
      "# for tweet in tweets:\n",
      "#     count += 1\n",
      "#     print count, tweet[\"text\"]\n",
      "    \n",
      "# next_page_url = search_url +response.json()[\"search_metadata\"][\"next_results\"]\n",
      "# response = requests.get(next_page_url, auth=oauth)\n",
      "\n",
      "# print 'Page 2'\n",
      "# for tweet in response.json()[\"statuses\"]:\n",
      "#     count+= 1\n",
      "#     print count,tweet[\"text\"]\n",
      "#     print"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!pip install tweepy"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\u001b[33mYou are using pip version 6.0.7, however version 6.0.8 is available.\r\n",
        "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Collecting tweepy\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  Downloading tweepy-3.3.0-py2.py3-none-any.whl\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Requirement already satisfied (use --upgrade to upgrade): requests>=2.4.3 in /usr/local/lib/python2.7/site-packages (from tweepy)\r\n",
        "Requirement already satisfied (use --upgrade to upgrade): six>=1.7.3 in /usr/local/lib/python2.7/site-packages (from tweepy)\r\n",
        "Requirement already satisfied (use --upgrade to upgrade): requests-oauthlib>=0.4.1 in /usr/local/lib/python2.7/site-packages (from tweepy)\r\n",
        "Requirement already satisfied (use --upgrade to upgrade): oauthlib>=0.6.2 in /usr/local/lib/python2.7/site-packages (from requests-oauthlib>=0.4.1->tweepy)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Installing collected packages: tweepy\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r\n",
        "Successfully installed tweepy-3.3.0\r\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import tweepy\n",
      "\n",
      "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
      "auth.set_access_token(access_token, access_secret)\n",
      "\n",
      "api = tweepy.API(auth)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# justins_tweets = api.user_timeline(\"sama\", count=100)\n",
      "\n",
      "# for tweet in justins_tweets:\n",
      "#     print tweet.text\n",
      "#     print"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "query = \"bitcoin\"\n",
      "max_tweets = 300\n",
      "\n",
      "search_results = tweepy.Cursor(api.search, q=query).items(max_tweets)\n",
      "\n",
      "# for i, tweet in enumerate(search_results):\n",
      "#     print i, tweet.text\n",
      "#     print"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import movie_reviews\n",
      "\n",
      "fileids = movie_reviews.fileids()[:100]\n",
      "\n",
      "doc_words = [movie_reviews.words(fileid) for fileid in fileids]\n",
      "documents = [' '.join(words) for words in doc_words]\n",
      "#print documents[:3]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(doc_words)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "100\n"
       ]
      }
     ],
     "prompt_number": 125
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.util import ngrams\n",
      "from nltk.tokenize import sent_tokenize, word_tokenize\n",
      "\n",
      "from collections import Counter\n",
      "from operator import itemgetter\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "stop = stopwords.words(\"english\")\n",
      "stop += ['.', ',', '(', ')', \"'\", '\"', \"?\", \"-\", \"!\", \"*\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 127
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "counter = Counter()\n",
      "\n",
      "for doc in documents:\n",
      "    word = TextBlob(doc).words\n",
      "    words = [w for w in words if w not in stop]\n",
      "    unigrams = ngrams(words, 1)\n",
      "    for gram in unigrams:\n",
      "        counter[gram] +=1\n",
      "        \n",
      "counter.most_common(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 133,
       "text": [
        "[((u'billy',), 400),\n",
        " ((u'fields',), 400),\n",
        " ((u'song',), 400),\n",
        " ((u'like',), 400),\n",
        " ((u'strawberry',), 300),\n",
        " ((u'lonely',), 300),\n",
        " ((u'leaving',), 300),\n",
        " ((u'beatles',), 300),\n",
        " ((u'sgt',), 300),\n",
        " ((u'know',), 300)]"
       ]
      }
     ],
     "prompt_number": 133
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#counter"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 144
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from scipy.sparse import coo_matrix\n",
      "\n",
      "#print len(documents)\n",
      "\n",
      "\n",
      "\n",
      "vectorizer = TfidfVectorizer(stop_words=\"english\", ngram_range=(1,1))\n",
      "\n",
      "\n",
      "\n",
      "doc_matrix = vectorizer.fit_transform(documents)\n",
      "\n",
      "vocab = vectorizer.vocabulary_\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 81
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "index_vocab = {col:word for word, col in vectorizer.vocabulary_.items()}\n",
      "\n",
      "\n",
      "def get_keywords_for_row(rownum):\n",
      "    row = doc_matrix.getrow(rownum)\n",
      "    tfidf_values = zip(row.data, row.indices)\n",
      "    for tfidf, column in sorted(tfidf_values, reverse=True)[:10]:\n",
      "        print tfidf, index_vocab[column]\n",
      "    \n",
      "\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n",
      "def alt_method(rownum):\n",
      "    row = doc_matrix.getrow(rownum)\n",
      "    for i in sorted(row.data, reverse=True)[:10]:\n",
      "        index_val = list(row.data).index(i)\n",
      "        col_val = row.indices[index_val]\n",
      "        index = index_vocab[col_val]\n",
      "        return i, index\n",
      "#sorted(row.indices, reverse=True)[:10]\n",
      "\n",
      "\n",
      "# for rownum in range(100):\n",
      "#     print alt_method(rownum)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 83
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n",
      "def alt_method_get_keyword(rownumber):\n",
      "    row = doc_matrix.getrow(rownumber)\n",
      "    \n",
      "    for idf_value in sorted(row.data, reverse=True)[:10]:\n",
      "    #for row in row.data:\n",
      "        index_number = list(row.data).index(idf_value)\n",
      "        #print col\n",
      "        col_val = row.indices[index_number]\n",
      "        index = index_vocab[col_val]\n",
      "        print index\n",
      "\n",
      "# for rownumber in range(10):\n",
      "#     print alt_method_get_keyword(rownum)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 66
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# for rownum in range(100):\n",
      "#     print get_keywords_for_row(rownum)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 84
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gatsby_vector = vectorizer.transform([GATSBY_TEXT])\n",
      "model.predict(gatsby_vector)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 94,
       "text": [
        "array(['neg'], \n",
        "      dtype='|S3')"
       ]
      }
     ],
     "prompt_number": 94
    }
   ],
   "metadata": {}
  }
 ]
}